{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping With Python and Requests-HTML\n",
    "\n",
    "In this Python for SEO tutorial, we will learn how to scrape a website with Python using the `Requests-HTML` library.\n",
    "\n",
    "(Code example included)\n",
    "\n",
    "@author: Jean-Christophe Chouinard: Technical SEO / Data Scientist > [LinkedIn](https://www.linkedin.com/in/jeanchristophechouinard/) > [@ChouinardJC](https://twitter.com/ChouinardJC) > Blog > [jcchouinard.com](https://www.jcchouinard.com/) > Complete Tutorial > [Web Scraping With Python and Requests-HTML](https://www.jcchouinard.com/web-scraping-with-python-and-requests-html/)\n",
    "\n",
    "## What is Web Scraping?\n",
    "*Web scraping* means the action of *parsing* the content of a webpage to extract specific information.\n",
    "\n",
    "*Parsing* means that you analyze a document to describe the syntax (i.e. the HTML structure). Without a *parser*, your HTML document will look like a single block of text.\n",
    "\n",
    "When you are scraping a website, you are asking the server to send you an HTML document that you *parse* to understand the building blocks (`<head>`,`<body>`,`<title>`,`<h1>`, etc.). Once the structure is understood, you can pull out any information that you want.\n",
    "\n",
    "## What is Requests-HTML Library?\n",
    "\n",
    "The `requests-HTML` library is a HTML parser that lets you use *CSS Selectors* and *XPath Selectors* to extract the information that you want from a web page.\n",
    "\n",
    "\n",
    "## Install and load Libraries\n",
    "\n",
    "In this tutorial, we will use the `requests` library to \"call\" the URL by making HTTP requests to servers, the `requests-HTML` library to *parse* the data, and the `pandas` library to work with the scraped information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install requests-HTML\n",
    "!pip install pandas\n",
    "!pip install regex\n",
    "!pip install urlparse4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the URL With requests.get()\n",
    "\n",
    "Use `HTMLSession()` to initialize the GET requests and the `.get()` function from `requests` to call the URL that you want to scrape.\n",
    "\n",
    "Just to make sure that there is no error, I will add a `try` and `except` statement to return an error in any case the code don't work.\n",
    "\n",
    "We will store the response in a variable called `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "url = \"https://www.searchenginejournal.com/introduction-to-python-seo-spreadsheets/342779/\"\n",
    "\n",
    "try:\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url)\n",
    "    \n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Scraping Functions\n",
    "\n",
    "The structure of the `requests-HTML` parsing call goes like this:\n",
    "\n",
    "`variable.attribute.function(*selector*, parameters)`\n",
    "\n",
    "The `variable` is the instance that you created using the `.get(url)` function.\n",
    "\n",
    "The `attribute` is the type of content that you want to extract (`html` / `lxml`).\n",
    "\n",
    "The `requests-HTML` parser also has many useful built-in `methods` for SEOs.\n",
    "\n",
    "* **links**: Get all links found on a page (anchors included);\n",
    "* **absolute_links**: Get all links found on a page  (anchors excluded);\n",
    "* **find()**: Find a specific element on a page with a CSS Selector;\n",
    "* **xpath()**: Get elements using Xpath function;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Title From the Page\n",
    "\n",
    "Here, we are going to use `find()` with the `html` attribute to \"find\" the `<title>` tag using the `'title'` *CSS Selector* and return a list of elements (`[<Element 'title' >]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title =  response.html.find('title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print to actual title, we need to use the index with the `text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as using the `first` parameter in the `function` in a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title =  response.html.find('title', first=True).text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Meta Description\n",
    "\n",
    "To extract the meta description from a page, we will use the `xpath()` function with the `//meta[@name=\"description\"]/@content` Xpath. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_desc =  response.html.xpath('//meta[@name=\"description\"]/@content')\n",
    "print(meta_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract All Links From a Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = response.html.absolute_links\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Information Using Class or ID\n",
    "\n",
    "You can extract any specific information from a page using the dot (`.`) notation to select a class, or the pound (`#`) notation to select the ID.\n",
    "\n",
    "Here we are going to extract the author name using the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamlet Batista\n"
     ]
    }
   ],
   "source": [
    "author = response.html.find('.post-author', first=True).text\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Canonical Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.searchenginejournal.com/introduction-to-python-seo-spreadsheets/342779/']\n"
     ]
    }
   ],
   "source": [
    "canonical = response.html.xpath(\"//link[@rel='canonical']/@href\")\n",
    "print(canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hreflang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "hreflang = response.html.xpath(\"//link[@rel='alternate']/@hreflang\")\n",
    "print(hreflang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Meta Robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOODP']\n"
     ]
    }
   ],
   "source": [
    "meta_robots = response.html.xpath(\"//meta[@name='ROBOTS']/@content\")\n",
    "print(meta_robots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Nested Information\n",
    "\n",
    "To extract information within a specific location you can dig down the DOM using CSS Selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nav_links = response.html.find('a.sub-m-cat span')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a for loop to loop through all the indices in the `nav_links` list and add the text to another list called `nav_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEO', 'PPC', 'CONTENT', 'SOCIAL', 'NEWS', 'ADVERTISE', 'MORE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav_links = []\n",
    "\n",
    "for i in range(len(get_nav_links)):\n",
    "    x = get_nav_links[i].text\n",
    "    nav_links.append(x)\n",
    "    \n",
    "nav_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a Subsection of a Page in a Variable\n",
    "\n",
    "If the content that you want to extract is always in a specific `<div>`, you can save the path in a variable to call it.\n",
    "\n",
    "Here, I will extract links that are in the actual content of a post by \"saving\" the `post-342779` article in a variable called `article`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = response.html.find('article.cis_post_item_initial.post-342779', first=True)\n",
    "article_links = article.xpath('//a/@href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Extract Broken Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Get Domain Name With urlparse\n",
    "url = \"https://www.jobillico.com/fr/partenaires-corporatifs\"\n",
    "parsed_url = urlparse(url)\n",
    "domain = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "\n",
    "# Get URL \n",
    "session = HTMLSession()\n",
    "r = session.get(url)\n",
    "\n",
    "# Extract Links\n",
    "jlinks = r.html.xpath('//a/@href')\n",
    "\n",
    "# Remove bad links and replace relative path for absolute path\n",
    "updated_links = []\n",
    "\n",
    "for link in jlinks:\n",
    "    if re.search(\".*@.*|.*javascript:.*|.*tel:.*\",link):\n",
    "        link = \"\"\n",
    "    elif re.search(\"^(?!http).*\",link):\n",
    "        link = domain + link\n",
    "        updated_links.append(link)\n",
    "    else:\n",
    "        updated_links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_links = []\n",
    "\n",
    "for link in updated_links:\n",
    "    print(link)\n",
    "    try: \n",
    "        requests.get(link, timeout=10).status_code\n",
    "        if requests.get(link, timeout=10).status_code != 200:\n",
    "            broken_links.append(link)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "broken_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "\n",
    "url = \"https://www.searchenginejournal.com/introduction-to-python-seo-spreadsheets/342779/\"\n",
    "\n",
    "try:\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url)\n",
    "except HTTPError as error:\n",
    "    print(error)\n",
    "\n",
    "    \n",
    "# Get Title\n",
    "title =  response.html.find('title', first=True).text\n",
    "\n",
    "#Get H1\n",
    "h1 =  response.html.find('h1', first=True).text\n",
    "\n",
    "#Get all Links\n",
    "links = response.html.absolute_links\n",
    "\n",
    "#Get Author using Class\n",
    "author = response.html.find('.post-author', first=True).text\n",
    "\n",
    "#Get Canonical Link\n",
    "canonical = response.html.xpath(\"//link[@rel='canonical']/@href\")\n",
    "\n",
    "#Get Hreflang\n",
    "hreflang = response.html.xpath(\"//link[@rel='alternate']/@hreflang\")\n",
    "\n",
    "#Get Meta Robots\n",
    "meta_robots = response.html.xpath(\"//meta[@name='ROBOTS']/@content\")\n",
    "\n",
    "#Get Navigational links using nested CSS Selector and For Loops\n",
    "get_nav_links = response.html.find('a.sub-m-cat span')\n",
    "\n",
    "nav_links = []\n",
    "\n",
    "for i in range(len(get_nav_links)):\n",
    "    x = get_nav_links[i].text\n",
    "    nav_links.append(x)\n",
    "    \n",
    "nav_links\n",
    "\n",
    "#Create a variable to extract dat from the actual article only.\n",
    "article = response.html.find('article.cis_post_item_initial.post-342779', first=True)\n",
    "article_links = article.xpath('//a/@href')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
